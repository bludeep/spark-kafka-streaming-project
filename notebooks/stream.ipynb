{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "def spark_init():\n",
    "    spark = ( SparkSession.builder.appName(\"KafkaStreamReader\")\n",
    "        .master(\"local[2]\")  \n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.6.0\")\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.executor.memory\", \"4g\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark \n",
    "\n",
    "\n",
    "\n",
    "def read_stream(spark, topic):\n",
    "    df = ( spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "        .option(\"subscribe\", topic)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load() \n",
    "                )\n",
    "    return df.select(\n",
    "        col(\"key\").cast(\"string\")\n",
    "        ,col(\"value\").cast(\"string\")\n",
    "        ,\"topic\"\n",
    "        ,\"partition\"\n",
    "        ,\"timestamp\"\n",
    "        ,\"offset\"\n",
    "    )\n",
    "\n",
    "def kafka_value_parsed(df):\n",
    "    schema = StructType([\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"District\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Town\", StringType(), True),\n",
    "    StructField(\"Type\", StringType(), True),\n",
    "    StructField(\"EnergyCertificate\", StringType(), True),\n",
    "    StructField(\"GrossArea\", DoubleType(), True),\n",
    "    StructField(\"TotalArea\", DoubleType(), True),\n",
    "    StructField(\"Parking\", DoubleType(), True),\n",
    "    StructField(\"HasParking\", BooleanType(), True),\n",
    "    StructField(\"Floor\", StringType(), True),\n",
    "    StructField(\"ConstructionYear\", DoubleType(), True),\n",
    "    StructField(\"EnergyEfficiencyLevel\", StringType(), True),\n",
    "    StructField(\"PublishDate\", TimestampType(), True),\n",
    "    StructField(\"Garage\", BooleanType(), True),\n",
    "    StructField(\"Elevator\", BooleanType(), True),\n",
    "    StructField(\"ElectricCarsCharging\", BooleanType(), True),\n",
    "    StructField(\"TotalRooms\", DoubleType(), True),\n",
    "    StructField(\"NumberOfBedrooms\", DoubleType(), True),\n",
    "    StructField(\"NumberOfWC\", DoubleType(), True),\n",
    "    StructField(\"ConservationStatus\", StringType(), True),\n",
    "    StructField(\"LivingArea\", DoubleType(), True),\n",
    "    StructField(\"LotSize\", DoubleType(), True),\n",
    "    StructField(\"BuiltArea\", DoubleType(), True),\n",
    "    StructField(\"NumberOfBathrooms\", DoubleType(), True)\n",
    "])\n",
    "    df = df.select(\n",
    "        \"key\"\n",
    "        ,from_json(col(\"value\"), schema).alias(\"parsed_data\")\n",
    "        ,col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "        ,\"topic\"\n",
    "        ,\"partition\"\n",
    "        ,\"offset\"\n",
    "    )\n",
    "    return df.select(\n",
    "        \"key\"\n",
    "        ,\"parsed_data.*\"\n",
    "        ,\"kafka_timestamp\"\n",
    "        ,\"offset\"\n",
    "        ,\"partition\"\n",
    "    )\n",
    "\n",
    "def clean_and_filter_kafka(df):\n",
    "    df = df.replace(['nan', 'NaN', 'null', ''], None)\n",
    "\n",
    "    df = df.withColumn(\"price\", \n",
    "        when(isnan(col(\"price\")) | isnull(col(\"price\")), None).otherwise(col(\"price\")))\n",
    "    \n",
    "    df = df.withColumn(\"main_area\",\n",
    "        when(col(\"type\") == \"Land\", \n",
    "             coalesce(col(\"lotsize\"), col(\"totalarea\"), col(\"grossarea\")))\n",
    "        .when(col(\"type\") == \"Farm\", \n",
    "             coalesce(col(\"lotsize\"), col(\"totalarea\"), col(\"grossarea\")))\n",
    "        .when(col(\"type\").isin([\"Apartment\", \"Studio\", \"Duplex\"]), \n",
    "             coalesce(col(\"livingarea\"), col(\"totalarea\"), col(\"grossarea\")))\n",
    "        .when(col(\"type\").isin([\"House\", \"Mansion\", \"Manor\", \"Estate\"]), \n",
    "             coalesce(col(\"livingarea\"), col(\"builtarea\"), col(\"totalarea\"), col(\"grossarea\")))\n",
    "        .when(col(\"type\").isin([\"Office\", \"Store\", \"Warehouse\", \"Industrial\", \"Storage\", \"Hotel\"]), \n",
    "             coalesce(col(\"totalarea\"), col(\"grossarea\"), col(\"builtarea\")))\n",
    "        .when(col(\"type\") == \"Garage\", \n",
    "             coalesce(col(\"builtarea\"), col(\"totalarea\"), col(\"grossarea\")))\n",
    "        .when(col(\"type\") == \"Building\", \n",
    "             coalesce(col(\"totalarea\"), col(\"grossarea\"), col(\"builtarea\")))\n",
    "        .otherwise(coalesce(col(\"totalarea\"), col(\"grossarea\"), col(\"livingarea\")))\n",
    "    ).filter(\n",
    "        col(\"price\").isNotNull() & \n",
    "        (col(\"price\") > 50000) &\n",
    "        col(\"main_area\").isNotNull() & \n",
    "        ~isnan(col(\"main_area\")) &\n",
    "        (col(\"main_area\") > 20) &\n",
    "        col(\"district\").isNotNull() &\n",
    "        col(\"type\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    return df.select(col(\"type\").alias(\"property_type\"), \"district\", \"price\", \"main_area\")\n",
    "\n",
    "def calculate_aggregations(df):\n",
    "    aggregate_df = df.groupBy([\"property_type\", \"district\"]) \\\n",
    "                .agg(\n",
    "                    round(avg(\"price\"), 2).alias(\"avg_price\")\n",
    "                    ,round(avg(\"main_area\"), 2).alias(\"avg_area\")\n",
    "                    ,count(\"*\").alias(\"total_ads\")\n",
    "                    ,round((sum(\"price\") / sum(\"main_area\")), 2).alias(\"avg_price_per_sqm\")\n",
    "                ).orderBy([\"district\", \"property_type\"])\n",
    "    return aggregate_df\n",
    "\n",
    "\n",
    "def write_to_postgres(df, epoch_id):\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://postgresql:5432/realestate\") \\\n",
    "            .option(\"dbtable\", \"real_estate_analytics\") \\\n",
    "            .option(\"user\", \"spark_user\") \\\n",
    "            .option(\"password\", \"spark_password\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка {e} при записи в Postgresql\")\n",
    "\n",
    "\n",
    "topic = \"csv-data\"\n",
    "spark = spark_init()\n",
    "stream = read_stream(spark, topic)\n",
    "parsed_df = kafka_value_parsed(stream)\n",
    "cleaned_kafka = clean_and_filter_kafka(parsed_df)\n",
    "aggregations = calculate_aggregations(cleaned_kafka)\n",
    "\n",
    "\n",
    "query = (\n",
    "    aggregations.writeStream\n",
    "        .option(\"checkpointLocation\", \"/tmp/kafka-checkpoint\")\n",
    "        .outputMode(\"complete\")\n",
    "        .foreachBatch(write_to_postgres)\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d248e70-de5d-4349-8099-068a17cc378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  URL: \u001b[0m\u001b[1mhttp://0.0.0.0:8501\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run dashboard.py --server.port 8501 --server.address 0.0.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0805630-c342-4d01-9499-86bedbb07f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
